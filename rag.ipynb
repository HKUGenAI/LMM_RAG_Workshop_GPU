{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG On Local Database\n",
    "In this part we use our prepared data for you: EngineeringHoistory3Books\n",
    "\n",
    "Make sure you have all the `xxx_text.parquet` and `xxx_image.parquet` files ready.\n",
    "This notebook demonstrates:\n",
    "- how to perform vector search on local database\n",
    "- perform augmented genAI based on searched results using Azure AI ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import environment packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from typing import List, Dict, Tuple\n",
    "from IPython.display import Image\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Local Text and Image Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textDB = None\n",
    "imageDB = None\n",
    "contentTensors = None\n",
    "captionTensors= None\n",
    "INPUT_PATH = None\n",
    "TEXT_K = None\n",
    "IMAGE_K = None\n",
    "\n",
    "def initDB():\n",
    "    global textDB, imageDB, contentTensors, captionTensors, INPUT_PATH, TEXT_K, IMAGE_K\n",
    "    if textDB is None or imageDB is None or contentTensors is None or captionTensors is None or INPUT_PATH is None or TEXT_K is None or IMAGE_K is None:\n",
    "        print(\"RAG reading database into memory\")\n",
    "        # IMPORTANT ************************  CONFIGURABLE VARIABLES\n",
    "        INPUT_PATH = \"EngineeringHistory3Books\" #You can change this to the path of your database\n",
    "        TEXT_K = 10\n",
    "        IMAGE_K = 5\n",
    "        # setup text database\n",
    "        textDB = pd.read_parquet(INPUT_PATH + \"_text.parquet\", engine=\"pyarrow\")\n",
    "        # normalize text vectors\n",
    "        contentTensors = F.normalize(torch.from_numpy(np.stack(textDB['contentVector'].to_numpy())), p=2, dim=1).to(torch.float32)\n",
    "        # setup image database\n",
    "        imageDB = pd.read_parquet(INPUT_PATH + \"_image.parquet\", engine=\"pyarrow\")\n",
    "        # normalize image caption vectors\n",
    "        captionTensors = F.normalize(torch.from_numpy(np.stack(imageDB['captionVector'].to_numpy())), p=2, dim=1).to(torch.float32)\n",
    "    print(\"RAG has finished reading database into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initDB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Search & RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform RAG, we first Embed our query to perform vector search on our local databases. Then we pass the retrieved relevant resutls to Azure ChatGPT to generate desired answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model variables\n",
    "embedding_client = AzureOpenAI(\n",
    "    api_key = os.getenv(\"EMBEDDING_OPENAI_API_KEY\"),\n",
    "    api_version = os.getenv(\"EMBEDDING_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint = os.getenv(\"EMBEDDING_OPENAI_API_ENDPOINT\")\n",
    ")\n",
    "embedding_model = os.getenv(\"EMBEDDING_DEPLOYMENT_NAME\")\n",
    "# Gen AI variables\n",
    "api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")  \n",
    "deployment_name = 'trygpt4o'  \n",
    "api_version = '2024-02-01'  # this might change in the future  \n",
    "client = AzureOpenAI(  \n",
    "    api_key=api_key,\n",
    "    api_version=api_version,  \n",
    "    base_url=f\"{api_base}/openai/deployments/{deployment_name}\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(messages: List[Dict]) -> Tuple[str, List[Dict]]:\n",
    "    # load global variables\n",
    "    global api_base, api_key, api_version, deployment_name, client\n",
    "    global embedding_client, embedding_model\n",
    "    global textDB, imageDB, contentTensors, captionTensors, INPUT_PATH, TEXT_K, IMAGE_K\n",
    "\n",
    "    # get latest message from user\n",
    "    query = messages[-1]['content']\n",
    "\n",
    "    # embed and normalize user query\n",
    "    queryVector = embedding_client.embeddings.create(input=[query], model=embedding_model).data[0].embedding\n",
    "    queryTensor = F.normalize(torch.tensor(queryVector), p=2, dim=0).to(torch.float32)\n",
    "\n",
    "    # search for text in textDB\n",
    "    text_cosine_similarities = torch.matmul(queryTensor, contentTensors.transpose(0,1))\n",
    "    topk_text_indices = torch.topk(text_cosine_similarities, k=TEXT_K).indices\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"Top K text indices:\")\n",
    "    print(textDB.iloc[topk_text_indices.numpy()])\n",
    "    text_search_results_for_GPT = str(textDB.iloc[topk_text_indices.numpy()][['id','content']].to_dict('records'))\n",
    "\n",
    "    # search for image in imageDB (score = query cosine similarty with caption * k number of text extracts + for each text extract's cosine similarity with caption)\n",
    "    image_search_score = torch.matmul(queryTensor, captionTensors.transpose(0,1)) * TEXT_K\n",
    "    for index in topk_text_indices.numpy():\n",
    "        image_search_score = image_search_score + torch.matmul(contentTensors[index], captionTensors.transpose(0,1))\n",
    "    topk_image_indices = torch.topk(image_search_score, k=IMAGE_K).indices\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"Top K image indices:\")\n",
    "    print(imageDB.iloc[topk_image_indices.numpy()])\n",
    "    image_search_results = imageDB.iloc[topk_image_indices.numpy()][['id', 'image', 'caption']].to_dict('records')\n",
    "    image_search_results_for_GPT = str(imageDB.iloc[topk_image_indices.numpy()][['id','caption']].to_dict('records'))\n",
    "\n",
    "    # Generate response\n",
    "    messages[-1]['content']=f'''Question: {query}\n",
    "    Sources: {text_search_results_for_GPT}\n",
    "    \n",
    "    Answer the question. Be specific in your answers. Answer ONLY with the facts listed in the list of sources above. If the question is not related to the sources, politely decline. If there isn't enough information from the sources, say you don't know. Do not generate answers that don't use the sources above. When you use information related to a particular source, include citation tags with the id as content like the example below. There can be multiple citation tags. Interleave images if it is relevant to your answer, relevancy can be determined from the provided captions. When you use images, include image tags with the id as content like the example below.\n",
    "    Example:\n",
    "    The Po Shan Road landslide incident occurred in 1972 and resulted in the deaths of 67 people. This landslide also affected part of the University of Hong Kong campus. The incident was one of two catastrophic landslides that year, which in total caused over 130 casualties and left more than 5,000 people homeless. The Po Shan Road landslide demolished a 12-storey apartment block. [citation:bookname_1_2]\n",
    "\n",
    "    [image:bookname_3_4]\n",
    "    \n",
    "    This tragic event, along with the Sau Mau Ping landslide, raised serious concerns about public safety concerning hillside development. These disasters led to the formation of an International Review Panel, which included experts such as Professor Sean Mackey and Professor Peter Lumb. The panel made significant recommendations on the management of landslide risks in Hong Kong. [citation:bookname_5_6] [citation:bookname_7_8]\n",
    "    '''\n",
    "    completion = client.chat.completions.create(\n",
    "        model = deployment_name,\n",
    "        messages = messages\n",
    "    )\n",
    "    chat_response = completion.choices[0].message.content\n",
    "    return(chat_response, image_search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a variable to store chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `user_prompt` to your own query and retrieve the response and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"tell me what are things author present in the work\" # Change your query here\n",
    "chat_history.append({'role': 'user', 'content': user_prompt}) \n",
    "response, image_list = query(chat_history)\n",
    "chat_history.append({'role' : 'assistant', 'content' : response})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is for displaying chat history and retrieved images.\\\n",
    "User queries are in Green.\\\n",
    "Responses are in Red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chat in chat_history:\n",
    "    if (chat['role'] == 'assistant'):\n",
    "        print('\\x1b[6;31m' + chat['content'] + '\\x1b[0m')\n",
    "    elif (chat['role'] == 'user'):\n",
    "        print('\\x1b[6;32m' + chat['content'] + '\\x1b[0m')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newRAG_0828",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
